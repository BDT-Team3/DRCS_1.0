
# Utilities
from data_templates import SIGNAL_CATEGORIES, NOISE_CATEGORIES, TEMPLATES, SYNONYMS

from sklearn.feature_extraction.text import TfidfVectorizer
from fastapi.responses import JSONResponse
from scipy.spatial.distance import cosine
from fastapi import FastAPI, Request
import numpy as np
import random
import logging
import pickle
import re
import os


# Configure logging with timestamps
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)

app = FastAPI()
ALL_LABELS = SIGNAL_CATEGORIES + NOISE_CATEGORIES


class ImprovedTfidfClassifier:
    """
    A lightweight, self-contained text classification model using TF-IDF vectorization
    and cosine similarity. It supports training on a set of labeled example texts and
    classifying new inputs by computing similarity to label embeddings.

    Label embeddings are generated by averaging the TF-IDF vectors of example texts
    for each label. This approach is fast, interpretable, and requires no deep learning.

    In the future this approach will be substituted by the deployment of a lighweight quantized/distilled
    deep learning model for better generalization.
    """
    def __init__(self):
        """
        Initialize the classifier by setting up the TF-IDF vectorizer and internal structures.

        The vectorizer is configured to:
        - Convert text to lowercase
        - Extract unigrams and bigrams
        - Remove English stop words
        - Limit the vocabulary to the top 5000 features (by frequency)

        Internal attributes:
        - `label_embeddings`: dictionary mapping labels to average TF-IDF vectors
        - `is_fitted`: flag indicating whether the model has been trained
        """
        self.vectorizer = TfidfVectorizer(
            lowercase=True,
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 2)
        )
        self.label_embeddings = {}
        self.is_fitted = False
    
    def preprocess_text(self, text):
        """Basic text cleaning: lowercase and remove punctuation"""
        # Simple preprocessing
        text = text.lower()
        # Remove special chars but keep spaces
        text = re.sub(r'[^\w\s]', '', text)
        return text
    
    def fit(self, texts_by_label):
        """Fit vectorizer with all texts and prepare label embeddings
        
        Args:
            texts_by_label: Dict mapping labels to lists of example texts
        """
        # Flatten all texts for vectorizer training
        all_texts = []
        for texts in texts_by_label.values():
            all_texts.extend(texts)
            
        # Fit vectorizer on all texts
        self.vectorizer.fit(all_texts)
        
        # Create label embeddings by averaging embeddings of their examples
        for label, texts in texts_by_label.items():
            if texts:
                # Preprocess texts
                preprocessed_texts = [self.preprocess_text(text) for text in texts]
                # Get TF-IDF for all examples of this label
                vectors = self.vectorizer.transform(preprocessed_texts).toarray()
                # Average them to get label embedding
                label_vector = vectors.mean(axis=0)
                # Normalize
                norm = np.linalg.norm(label_vector)
                if norm > 0:
                    label_vector = label_vector / norm
                self.label_embeddings[label] = label_vector
                
        self.is_fitted = True
        return self
    
    def get_embedding(self, text):
        """
        Generate a normalized TF-IDF embedding for a new input text.

        Args:
            text (str): Input text to vectorize.

        Returns:
            np.ndarray: Normalized embedding vector.
        """
        if not self.is_fitted:
            raise ValueError("Vectorizer not fitted")

        processed_text = self.preprocess_text(text)
        vector = self.vectorizer.transform([processed_text])
        embedding = vector.toarray()[0]
        
        # Normalize
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
            
        return embedding
    
    def classify(self, text, candidate_labels):
        """
        Compute cosine similarity between input text and each candidate label.

        Args:
            text (str): Input message to classify.
            candidate_labels (list): List of label names to compare against.

        Returns:
            dict: Ranked list of labels with similarity scores.
        """
        text_embedding = self.get_embedding(text)
        scores = []
        
        for label in candidate_labels:
            if label not in self.label_embeddings:
                # Skip labels we don't have embeddings for
                continue
                
            label_embedding = self.label_embeddings[label]
            similarity = 1 - cosine(text_embedding, label_embedding)
            scores.append(similarity)
            
        # Create and sort results
        label_scores = list(zip(candidate_labels, scores))
        label_scores.sort(key=lambda x: x[1], reverse=True)
        
        return {
            "sequence": text,
            "labels": [label for label, _ in label_scores],
            "scores": [float(score) for _, score in label_scores]
        }
        
    
    def save(self, filepath):
        """Persist the trained classifier to disk."""
        with open(filepath, 'wb') as f:
            pickle.dump(self, f)
    
    @classmethod
    def load(cls, filepath):
        """Load a previously saved classifier from file."""
        with open(filepath, 'rb') as f:
            return pickle.load(f)

def fill_template(template: str) -> str:
    """
    Fill in a template string with random synonyms from the SYNONYMS dictionary.

    Args:
        template (str): A string with placeholders like {incident}, {place}, etc.

    Returns:
        str: A fully populated sentence.
    """
    return template.format(**{
        key: random.choice(values)
        for key, values in SYNONYMS.items()
        if f"{{{key}}}" in template
    })

# Generates synthetic texts from all the defined templates
def generate_training_texts_by_label():
    """
    Generate a small synthetic dataset for each label by populating templates.

    Returns:
        dict: Maps each label to a list of generated training examples.
    """
    texts_by_label = {}
    for label, templates in TEMPLATES.items():
        texts_by_label[label] = []
        for template in templates:
            # Generate multiple variants
            texts_by_label[label].extend([fill_template(template) for _ in range(3)])
    return texts_by_label


# Initialize classifier - reuse if saved, otherwise generate synthetic training data
classifier_path = 'tfidf_classifier.pkl'
if os.path.exists(classifier_path):
    try:
        classifier = ImprovedTfidfClassifier.load(classifier_path)
    except (AttributeError, ModuleNotFoundError, pickle.PicklingError):
        # If loading fails, create a new classifier
        print("Failed to load existing classifier, creating new one...")
        classifier = ImprovedTfidfClassifier()
        texts_by_label = generate_training_texts_by_label()
        classifier.fit(texts_by_label)
        classifier.save(classifier_path)
else:
    # Initialize and train with synthetic data if no saved model exists
    classifier = ImprovedTfidfClassifier()
    texts_by_label = generate_training_texts_by_label()
    classifier.fit(texts_by_label)
    classifier.save(classifier_path)

@app.get("/health")
async def health_check():
    """Simple health check endpoint to verify the service is running."""
    return {"status": "ok"}

@app.post("/classify")
async def classify_message(request: Request):
    """
    REST endpoint that classifies incoming messages using the trained TF-IDF model.
    Messages arrive in batch to limit Http calls per minute and protocol saturation.

    Expects a JSON list of dictionaries, each with:
    - 'text': the message string
    - 'labels': optional list of labels to classify against
    - other metadata fields like 'timestamp', 'unique_msg_id', etc.

    Returns:
        JSONResponse: List of results, each with:
            - classified category
            - original message
            - metadata fields
    """
    results = []
    data = await request.json()
    for msg in data:
        message = msg.get("text", "")
        labels = msg.get("labels", ALL_LABELS)  # default to ALL_LABELS if none provided
        
        if not message:
            results.append({
                "error": "Missing 'text' field",
                "input": msg
            })
            continue
            
        # Pass the message directly to classify
        result = classifier.classify(message, candidate_labels=labels)
        
        try:
            nlp_msg = {
                "message": result["sequence"],
                "category": result["labels"][0],
                "unique_msg_id": msg["unique_msg_id"],
                "macroarea_id": msg["macroarea_id"],
                "microarea_id": msg["microarea_id"],
                "latitude": msg["latitude"],
                "longitude": msg["longitude"],
                "timestamp": msg["timestamp"]
            }
            results.append(nlp_msg)
        except Exception as e:
            print(f"Failed to parse message metadata to nlp output, cause: {e}")
            results.append({
                "error": str(e),
                "input": msg
            })
            
    return JSONResponse(content=results)

